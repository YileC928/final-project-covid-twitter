{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:10:00.011484Z",
     "start_time": "2021-03-11T02:09:59.994256Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'preprocessor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-16c4d6b3e4e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'preprocessor'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import preprocessor as p\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:10:00.924363Z",
     "start_time": "2021-03-11T02:10:00.347622Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp = spacy.load(\"en\")\n",
    "except OSError:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:10:01.151109Z",
     "start_time": "2021-03-11T02:10:01.114500Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for line in open('data/cdc_twitter_covid.json', 'r', encoding='utf-8'):\n",
    "    tweets.append(json.loads(line))\n",
    "    \n",
    "df = pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:10:01.349338Z",
     "start_time": "2021-03-11T02:10:01.335607Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:10:01.964859Z",
     "start_time": "2021-03-11T02:10:01.947748Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[['id', 'date', 'time', 'username', 'tweet', 'mentions','urls', 'photos', 'hashtags', 'link', 'quote_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:10:02.608520Z",
     "start_time": "2021-03-11T02:10:02.590533Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_df = df[['id', 'date', 'time', 'username', 'tweet', 'hashtags']]\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:10:03.184782Z",
     "start_time": "2021-03-11T02:10:03.146562Z"
    }
   },
   "outputs": [],
   "source": [
    "print('--- Print the Basic Info of the data ----')\n",
    "print(tweets_df.info())\n",
    "print(tweets_df.shape)\n",
    "\n",
    "print('--- Print the Head/Tail of the data -----')\n",
    "print(tweets_df.head())\n",
    "print('------------------------')\n",
    "print(tweets_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:10:03.776570Z",
     "start_time": "2021-03-11T02:10:03.538418Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove URLs, emojis, smileys, mentions, hashtags, and reserved words\n",
    "for i,v in enumerate(tweets_df['tweet']):\n",
    "    p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.SMILEY, p.OPT.MENTION, p.OPT.HASHTAG, p.OPT.RESERVED) # options: p.OPT.URL, p.OPT.MENTION, p.OPT.HASHTAG,\n",
    "                                                                                                #p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.SMILEY,\n",
    "                                                                                                #p.OPT.NUMBER\n",
    "    tweets_df.loc[i, 'tweet'] = p.clean(v)\n",
    "    tweets_df.loc[i, 'tweet'] = tweets_df.loc[i, \"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:10:03.978589Z",
     "start_time": "2021-03-11T02:10:03.973596Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_df.loc[0, \"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:10:04.541922Z",
     "start_time": "2021-03-11T02:10:04.513871Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove extra white spaces, punctuation and apply lower casing\n",
    "tweets_df['tweet'] = tweets_df['tweet'].str.lower().str.replace('[^\\w\\s]',' ').str.replace('\\s\\s+', ' ')\n",
    "tweets_df.loc[0, \"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:10:05.094071Z",
     "start_time": "2021-03-11T02:10:05.074940Z"
    }
   },
   "outputs": [],
   "source": [
    "# lemmatize and tokenize\n",
    "def word_tokenize(word_list, model=nlp, MAX_LEN=1500000):   \n",
    "    tokenized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list]) \n",
    "    # since we're only tokenizing, I remove RAM intensive operations and increase max text size\n",
    "\n",
    "    model.max_length = MAX_LEN\n",
    "    doc = model(word_list, disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "    \n",
    "    for token in doc:\n",
    "        if not token.is_punct and len(token.text.strip()) > 0:\n",
    "            tokenized.append(token.text)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:10:05.675875Z",
     "start_time": "2021-03-11T02:10:05.643874Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalizeTokens(word_list, extra_stop=[], model=nlp, lemma=True, MAX_LEN=1500000):\n",
    "    normalized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list]) \n",
    "\n",
    "    # since we're only normalizing, I remove RAM intensive operations and increase max text size\n",
    "\n",
    "    model.max_length = MAX_LEN\n",
    "    doc = model(word_list.lower(), disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "\n",
    "    if len(extra_stop) > 0:\n",
    "        for stopword in extra_stop:\n",
    "            lexeme = nlp.vocab[stopword]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "    # check if we want lemmas or not earlier to avoid checking every time we loop\n",
    "    if lemma:\n",
    "        for w in doc:\n",
    "            # if it's not a stop word or punctuation mark, add it to our article\n",
    "            if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # we add the lematized version of the word\n",
    "                normalized.append(str(w.lemma_))\n",
    "    else:\n",
    "        for w in doc:\n",
    "            # if it's not a stop word or punctuation mark, add it to our article\n",
    "            if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # we add the lematized version of the word\n",
    "                normalized.append(str(w.text.strip()))\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:10:06.344188Z",
     "start_time": "2021-03-11T02:10:06.196226Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_df['tokenized_tweet'] = tweets_df['tweet'].apply(lambda x: word_tokenize(x))\n",
    "tweets_df['normalized_tokens'] = tweets_df['tokenized_tweet'].apply(lambda x: normalizeTokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:10:07.331408Z",
     "start_time": "2021-03-11T02:10:07.291411Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T20:04:40.310921Z",
     "start_time": "2021-03-08T20:04:40.304785Z"
    }
   },
   "outputs": [],
   "source": [
    "# save tweets as txt file\n",
    "#tweets_df[\"tweet\"].to_csv(\"tweet\" + '.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:10:14.801233Z",
     "start_time": "2021-03-11T02:10:14.786234Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_df['year'] = tweets_df['date'].apply(lambda x: x.split('-')[0])\n",
    "tweets_df['month'] = tweets_df['date'].apply(lambda x: x.split('-')[1])\n",
    "tweets_df['date'] = tweets_df['date'].apply(lambda x: x.split('-')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:10:15.337641Z",
     "start_time": "2021-03-11T02:10:15.299693Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:10:28.094954Z",
     "start_time": "2021-03-11T02:10:28.074952Z"
    }
   },
   "outputs": [],
   "source": [
    "%store tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:10:34.706108Z",
     "start_time": "2021-03-11T02:10:34.674440Z"
    }
   },
   "outputs": [],
   "source": [
    "#save cleaned texts as txt file\n",
    "tweets_df.to_csv(\"cleaned_tweets\" + '.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:19:36.709562Z",
     "start_time": "2021-03-11T02:19:36.651523Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets01 = tweets_df[((tweets_df[\"month\"] == '02') |(tweets_df[\"month\"] == '03')) & (tweets_df[\"year\"] == '2020')]\n",
    "tweets01 = tweets01.reset_index(drop=True)\n",
    "tweets01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:19:51.987773Z",
     "start_time": "2021-03-11T02:19:51.954559Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets02 = tweets_df[((tweets_df[\"month\"] == '04') |(tweets_df[\"month\"] == '05') | (tweets_df[\"month\"] == '06')) & (tweets_df[\"year\"] == '2020')]\n",
    "tweets02 = tweets02.reset_index(drop=True)\n",
    "tweets02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:20:24.365976Z",
     "start_time": "2021-03-11T02:20:24.330993Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets03 = tweets_df[((tweets_df[\"month\"] == '07') |(tweets_df[\"month\"] == '08') | (tweets_df[\"month\"] == '09')) & (tweets_df[\"year\"] == '2020')]\n",
    "tweets03 = tweets03.reset_index(drop=True)\n",
    "tweets03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:20:42.165505Z",
     "start_time": "2021-03-11T02:20:42.139502Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets04 = tweets_df[((tweets_df[\"month\"] == '10') |(tweets_df[\"month\"] == '11') | (tweets_df[\"month\"] == '12')) & (tweets_df[\"year\"] == '2020')]\n",
    "tweets04 = tweets04.reset_index(drop=True)\n",
    "tweets04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:20:57.147470Z",
     "start_time": "2021-03-11T02:20:57.108813Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets0121 = tweets_df[tweets_df[\"year\"] == '2021']\n",
    "tweets0121 = tweets0121.reset_index(drop=True)\n",
    "tweets0121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:21:01.746072Z",
     "start_time": "2021-03-11T02:21:01.738499Z"
    }
   },
   "outputs": [],
   "source": [
    "%store tweets0121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:21:02.544831Z",
     "start_time": "2021-03-11T02:21:02.539589Z"
    }
   },
   "outputs": [],
   "source": [
    "%store tweets01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:21:03.208992Z",
     "start_time": "2021-03-11T02:21:03.202981Z"
    }
   },
   "outputs": [],
   "source": [
    "%store tweets02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:21:03.929176Z",
     "start_time": "2021-03-11T02:21:03.922170Z"
    }
   },
   "outputs": [],
   "source": [
    "%store tweets03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:21:04.497782Z",
     "start_time": "2021-03-11T02:21:04.490727Z"
    }
   },
   "outputs": [],
   "source": [
    "%store tweets04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:10:51.592766Z",
     "start_time": "2021-03-11T02:10:51.434647Z"
    }
   },
   "outputs": [],
   "source": [
    "cdc_tweets = []\n",
    "for line in open('data/cdc_twitter_since_2020.json', 'r', encoding='utf-8'):\n",
    "    cdc_tweets.append(json.loads(line))\n",
    "    \n",
    "cdc_df = pd.DataFrame(cdc_tweets)\n",
    "cdc_df = cdc_df[['id', 'date', 'time', 'tweet']]\n",
    "cdc_df['year'] = cdc_df['date'].apply(lambda x: x.split('-')[0])\n",
    "cdc_df['month'] = cdc_df['date'].apply(lambda x: x.split('-')[1])\n",
    "cdc_df['date'] = cdc_df['date'].apply(lambda x: x.split('-')[2])\n",
    "cdc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:25:18.257864Z",
     "start_time": "2021-03-11T02:25:18.243846Z"
    }
   },
   "outputs": [],
   "source": [
    "all01 = cdc_df[((cdc_df[\"month\"] == '02') |(cdc_df[\"month\"] == '03')) & (cdc_df[\"year\"] == '2020')]\n",
    "all01 = all01.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T03:31:25.294679Z",
     "start_time": "2021-03-11T03:31:25.269646Z"
    }
   },
   "outputs": [],
   "source": [
    "all01 = all01.drop(all01[all01['date']<\"25\"].index)\n",
    "all01 = all01.reset_index(drop=True)\n",
    "all01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:27:47.202040Z",
     "start_time": "2021-03-11T02:27:47.178041Z"
    }
   },
   "outputs": [],
   "source": [
    "all02 = cdc_df[((cdc_df[\"month\"] == '04') |(cdc_df[\"month\"] == '05') |(cdc_df[\"month\"] == '06')) & (cdc_df[\"year\"] == '2020')]\n",
    "all02 = all02.reset_index(drop=True)\n",
    "all02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:28:21.995641Z",
     "start_time": "2021-03-11T02:28:21.977639Z"
    }
   },
   "outputs": [],
   "source": [
    "all03 = cdc_df[((cdc_df[\"month\"] == '07') |(cdc_df[\"month\"] == '08') |(cdc_df[\"month\"] == '09')) & (cdc_df[\"year\"] == '2020')]\n",
    "all03 = all03.reset_index(drop=True)\n",
    "all03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:28:58.508835Z",
     "start_time": "2021-03-11T02:28:58.488834Z"
    }
   },
   "outputs": [],
   "source": [
    "all04 = cdc_df[((cdc_df[\"month\"] == '10') |(cdc_df[\"month\"] == '11') |(cdc_df[\"month\"] == '12')) & (cdc_df[\"year\"] == '2020')]\n",
    "all04 = all04.reset_index(drop=True)\n",
    "all04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T02:29:32.426722Z",
     "start_time": "2021-03-11T02:29:32.401172Z"
    }
   },
   "outputs": [],
   "source": [
    "all0121 = cdc_df[cdc_df[\"year\"] == '2021']\n",
    "all0121 = all0121.reset_index(drop=True)\n",
    "all0121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T03:51:53.411108Z",
     "start_time": "2021-03-11T03:51:53.188091Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams.update({'font.family' : 'Times New Roman', 'font.size': 23})\n",
    "fig = plt.figure(figsize = (12,8))\n",
    "label_list = ['2020 02.25-03', '2020 04-06', '2020 07-09', '2020 10-12', '2021 01-02.22']\n",
    "num_list1 = [len(tweets01), len(tweets02), len(tweets03), len(tweets04), len(tweets0121)]\n",
    "num_list2 = [len(all01)-len(tweets01), len(all02)-len(tweets02), len(all03)-len(tweets03), \n",
    "             len(all04)-len(tweets04), len(all0121)-len(tweets0121)]\n",
    "num_list3 = [len(all01), len(all02), len(all03), len(all04), len(all0121)]\n",
    "x = range(len(num_list1))\n",
    "rects1 = plt.bar(x=x, height=num_list1, width=0.45, alpha=0.8, label=\"COVID-related\")\n",
    "rects2 = plt.bar(x=x, height=num_list2, width=0.45, label=\"Total\", bottom=num_list1)\n",
    "\n",
    "count = 0\n",
    "count2 = 0\n",
    "for i in num_list1:\n",
    "    plt.text(count,i+0.5, str('{:.2f}'.format(num_list1[count]/num_list3[count] *100)) +'%', \\\n",
    "    ha='center') #位置，高度，内容，居中\n",
    "    count+=1 #增加百分比\n",
    "for i in num_list3: \n",
    "    plt.text(count2,i+20, str('{}'.format(num_list3[count2])), ha='center')\n",
    "    count2+=1\n",
    "    \n",
    "plt.ylim(0, 900)\n",
    "plt.ylabel(\"Number\")\n",
    "plt.xticks(x, label_list)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.title(\"Number of CDC Tweets by Quarters\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
